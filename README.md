# End-to-End Data Pipeline using Databricks Delta Live Tables

## ğŸš€ Project Overview
This project demonstrates how to design and implement a **real-world data engineering pipeline** using **Databricks Delta Live Tables (DLT)** on Azure.  
It follows the **Medallion Architecture (Bronze â†’ Silver â†’ Gold)** to enable **incremental**, **scalable**, and **reliable data processing** for an **e-commerce sales analytics** use case.

The goal of this project is to transform raw transactional data into **clean, business-ready insights** through automated data lineage, SCD management, and DLT pipeline orchestration.

---

## ğŸ§© Architecture
```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Raw Data Sources      â”‚
                â”‚ (Products, Customers,   â”‚
                â”‚   and Sales Data)       â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                      Bronze Layer
                  (Data Ingestion & Cleaning)
                           â”‚
                      Silver Layer
          (Data Transformation & Enrichment)
                           â”‚
                      Gold Layer
        (Business Aggregations & Analytics Models)
                           â”‚
                   Visualization Tools
              (Power BI, Tableau, etc.)
```

---

## âš™ï¸ Technologies & Tools Used
| Category | Tools / Technologies |
|-----------|----------------------|
| Platform | Azure Databricks |
| Framework | Delta Live Tables (DLT) |
| Language | Python, PySpark |
| Storage | Azure Data Lake Gen2 |
| Processing | Spark Structured Streaming |
| Data Modeling | Medallion Architecture (Bronze, Silver, Gold) |
| Change Tracking | SCD Type 1 & Type 2 |
| Version Control | GitHub |
| Visualization | Power BI / Databricks SQL |

---

## ğŸ“‚ Project Structure
```
DLT_Root/
â”œâ”€â”€ explorations/               # Exploratory analysis scripts
â”‚   â””â”€â”€ sample_exploration.py
â”œâ”€â”€ utilities/                  # Utility and helper functions
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ transformations_SorceCode/
â”‚   â”œâ”€â”€ bronze/                 # Ingestion layer
â”‚   â”‚   â”œâ”€â”€ ingestion_products.py
â”‚   â”‚   â”œâ”€â”€ ingestion_customers.py
â”‚   â”‚   â””â”€â”€ ingestion_sales.py
â”‚   â”œâ”€â”€ silver/                 # Transformation layer
â”‚   â”‚   â”œâ”€â”€ transform_products.py
â”‚   â”‚   â”œâ”€â”€ transform_customers.py
â”‚   â”‚   â””â”€â”€ transform_sales.py
â”‚   â”œâ”€â”€ gold/                   # Business & analytics layer
â”‚       â”œâ”€â”€ dim_products.py
â”‚       â”œâ”€â”€ dim_customers.py
â”‚       â”œâ”€â”€ fact_sales.py
â”‚       â””â”€â”€ business_sales.py
â”‚                    
â”‚        
â”‚     
â””â”€â”€ README.md
```

---

## ğŸ” Pipeline Flow
1. **Bronze Layer (Raw Data Ingestion)**  
   - Ingests raw data from multiple sources (CSV/Parquet/API).  
   - Performs schema validation and data cleaning.  

2. **Silver Layer (Data Transformation)**  
   - Applies business logic, joins datasets, and ensures referential integrity.  
   - Implements **incremental and streaming transformations**.  

3. **Gold Layer (Business & Analytics)**  
   - Creates **fact and dimension tables** for sales analysis.  
   - Handles **SCD Type 1 and Type 2** logic for product and customer changes.  
   - Prepares final curated tables for reporting dashboards.  

---

## ğŸ§± Key Features
- âœ… Delta Live Tables for **automated pipeline management**
- âœ… Supports **incremental batch and streaming loads**
- âœ… Implements **SCD Type 1 and 2** for change tracking
- âœ… Follows **Medallion Architecture** best practices
- âœ… Easy integration with **Power BI / Databricks SQL**
- âœ… Modular, reusable Python code structure

---

## ğŸ§ª How to Run the Project in Databricks
1. Clone this repository into your Databricks workspace:
   ```bash
   git clone https://github.com/<your-username>/DLT_Root.git
   ```
2. Import the project folder into Databricks Repos.  
3. Open each notebook/script under `transformations_SorceCode/`.  
4. Create a new **Delta Live Tables Pipeline** in Databricks:
   - Pipeline Source: this repo path  
   - Target: your database name (e.g., `databricks_catalog.gold`)  
   - Storage Location: Azure Data Lake path  
5. Run the pipeline and monitor:
   - Lineage view  
   - Job metrics  
   - Error handling and recovery  

---

## ğŸŒ Real-World Use Case
This project simulates a real-world **e-commerce data pipeline** that:
- Tracks daily product sales across multiple regions.  
- Updates customer and product master data with history tracking.  
- Produces business insights such as revenue trends, top products, and customer lifetime value.  

---

## ğŸ“Š Lineage and Monitoring
DLT provides an automatic **data lineage graph**, enabling you to visualize dependencies across tables:
```
bronze_sales â†’ silver_sales â†’ gold_fact_sales
bronze_customers â†’ silver_customers â†’ gold_dim_customers
bronze_products â†’ silver_products â†’ gold_dim_products
```

---

## ğŸ§° Future Enhancements
- Add CI/CD pipeline for Databricks Repos  
- Integrate Data Quality checks using **Expectations**  
- Extend to multi-source ingestion (API, Kafka)  
- Add Databricks Workflows for orchestration  

---

## ğŸ‘¨â€ğŸ’» Author
**Rahul Mandaviya**  
ğŸ“ Data Engineer | Azure | Databricks | Power BI  
ğŸ”— [LinkedIn Profile](https://www.linkedin.com/in/rahul-mandaviya/)
